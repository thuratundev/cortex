import os
import json
from dotenv import load_dotenv
import numpy as np
from google import genai
from google.genai.types import EmbedContentConfig

load_dotenv()
GOOGLE_API_KEY = os.getenv('API_KEY')


MODEL_NAME = "gemini-embedding-001" 

# --- File Paths ---
EMBEDDED_FILE_NAME = "./data/embedded_data.json"

def get_query_embedding(query_text: str, model) -> list[float] | None:
   
    if not GOOGLE_API_KEY:
        raise ValueError("FATAL ERROR: GOOGLE_API_KEY is not set. Please set the environment variable or paste the key directly in the script.")
    client = genai.Client(api_key=GOOGLE_API_KEY)

    try:
           
        response = client.models.embed_content(
                model=MODEL_NAME,
                contents=query_text,
                config=EmbedContentConfig(
                    task_type="RETRIEVAL_QUERY",  # Optional
                ),
            )        

        return response.embeddings[0].values
    except Exception as e:
        print(f"á€™á€±á€¸á€á€½á€”á€ºá€¸á€€á€­á€¯ embedding á€–á€”á€ºá€á€®á€¸á€›á€¬á€á€½á€„á€º á€¡á€™á€¾á€¬á€¸á€¡á€šá€½á€„á€ºá€¸á€–á€¼á€…á€ºá€•á€±á€«á€ºá€•á€«á€á€Šá€º: {e}")
        return None
    
def cosine_similarity(vec1, vec2):

    dot_product = np.dot(vec1, vec2)
    # np.linalg.norm á€€ vector á€á€…á€ºá€á€¯á€›á€²á€· á€¡á€œá€»á€¬á€¸ (magnitude) á€€á€­á€¯ á€›á€¾á€¬á€•á€±á€¸á€á€¬á€•á€«
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    
    
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
        
    return dot_product / (norm_vec1 * norm_vec2)    

def find_most_similar_result(user_query: str, top_k: int = 5) -> list[dict] | None:
   
    try:
        with open(EMBEDDED_FILE_NAME, 'r', encoding='utf-8') as f:
            data_embeddings = json.load(f)
        print(f" Successfully read embedding file ({len(data_embeddings)} entries)")
    except FileNotFoundError:
        print(f"'{EMBEDDED_FILE_NAME}' file not found. Please run the embedding generator script first.")
        return

    print(f"\nConverting user query '{user_query}' to vector...")
    query_vector = get_query_embedding(user_query, MODEL_NAME)
    if not query_vector:
        return

  
    print("Calculating similarity scores...")
    scores = []
    for proc in data_embeddings:
        proc_name = proc["ReportName"]
        proc_description = proc["Description"]
        proc_vector = proc["Vector"]
        score = cosine_similarity(query_vector, proc_vector)
        scores.append({"ReportName": proc_name,"Description": proc_description,"Score": score})

    sorted_scores = sorted(scores, key=lambda x: x["Score"], reverse=True)

    max_score = sorted_scores[0]['Score'] if sorted_scores else 0.0
    for found in sorted_scores:
        if found['Score'] == max_score:
            print(f"Report Name: {found['ReportName']}, Score: {found['Score']:.4f}")
            break
        
    #for found in sorted_scores: 
    #print(f"Report Name: {found['ReportName']}, Score: {found['Score']:.4f}")

    return sorted_scores[:top_k]

def find_best_response_with_gemini(user_query : str,similar_results: list[dict]) -> str:
    
    print(f"Finding best response for query: {user_query}")
    
    if not similar_results:
        return "No similar results found."

    context_for_prompt = ""
    for i, report in enumerate(similar_results):
        report_name = report['ReportName']
        description = report["Description"]
        
        context_for_prompt += f"--- Report {i+1} ---\n"
        context_for_prompt += f"Report á€¡á€™á€Šá€º: {report_name}\n"
        context_for_prompt += f"Report á€–á€±á€¬á€ºá€•á€¼á€á€»á€€á€º: {description}\n\n"

    
    prompt = f"""
        á€á€„á€ºá€á€Šá€º ERP á€…á€”á€…á€ºá€¡á€á€½á€€á€º á€¡á€‘á€°á€¸á€•á€¼á€¯á€™á€±á€¸á€á€½á€”á€ºá€¸á€™á€»á€¬á€¸á€–á€¼á€±á€†á€­á€¯á€•á€±á€¸á€á€±á€¬ Retrieval-Augmented Generation (RAG) AI á€á€…á€ºá€¦á€¸á€–á€¼á€…á€ºá€•á€«á€á€Šá€ºá‹
        á€á€„á€ºá á€¡á€“á€­á€€á€á€¬á€á€”á€ºá€™á€¾á€¬ á€¡á€±á€¬á€€á€ºá€á€½á€„á€ºá€•á€±á€¸á€‘á€¬á€¸á€á€±á€¬ "á€›á€¾á€¬á€–á€½á€±á€á€½á€±á€·á€›á€¾á€­á€‘á€¬á€¸á€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€º (Context)" á€€á€­á€¯á€á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á "á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€°á á€™á€±á€¸á€á€½á€”á€ºá€¸" á€€á€­á€¯ á€á€­á€€á€»á€…á€½á€¬á€–á€¼á€±á€†á€­á€¯á€›á€”á€º á€–á€¼á€…á€ºá€á€Šá€ºá‹

        **á€œá€­á€¯á€€á€ºá€”á€¬á€›á€”á€º á€…á€Šá€ºá€¸á€™á€»á€‰á€ºá€¸á€™á€»á€¬á€¸:**

        1.  **Context á€€á€­á€¯á€á€¬ á€¡á€á€¼á€±á€á€¶á€•á€«:** á€á€„á€ºáá€¡á€–á€¼á€±á€á€Šá€º á€•á€±á€¸á€‘á€¬á€¸á€á€±á€¬ "á€›á€¾á€¬á€–á€½á€±á€á€½á€±á€·á€›á€¾á€­á€‘á€¬á€¸á€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€º (Context)" á á€˜á€±á€¬á€„á€ºá€¡á€á€½á€„á€ºá€¸áŒá€á€¬ á€›á€¾á€­á€›á€™á€Šá€ºá‹ á€á€„á€ºá á€€á€­á€¯á€šá€ºá€•á€­á€¯á€„á€ºá€¡á€á€­á€•á€Šá€¬ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€¡á€á€¼á€¬á€¸á€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€›á€„á€ºá€¸á€™á€¼á€…á€ºá€™á€»á€¬á€¸á€€á€­á€¯ **á€œá€¯á€¶á€¸á€á€¡á€á€¯á€¶á€¸á€™á€•á€¼á€¯á€•á€«á€”á€¾á€„á€·á€º**á‹
        2.  **á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€ºá€”á€¾á€„á€·á€º á€á€­á€€á€»á€•á€«:** á€™á€±á€¸á€á€½á€”á€ºá€¸á€€á€­á€¯ á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€ºá€–á€¼á€±á€†á€­á€¯á€•á€«á‹ á€¡á€–á€¼á€±á€€á€­á€¯ á€á€­á€¯á€á€­á€¯áŠ á€œá€­á€¯á€›á€„á€ºá€¸á€”á€¾á€„á€·á€º á€á€­á€€á€»á€¡á€±á€¬á€„á€ºá€›á€±á€¸á€•á€«á‹ á€™á€œá€­á€¯á€¡á€•á€ºá€á€±á€¬ á€”á€­á€’á€«á€”á€ºá€¸á€•á€»á€­á€¯á€¸á€á€¼á€„á€ºá€¸áŠ á€”á€¾á€¯á€á€ºá€†á€€á€ºá€á€¼á€„á€ºá€¸ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€¡á€•á€­á€¯á€…á€€á€¬á€¸á€™á€»á€¬á€¸ á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€›á€”á€ºá€™á€œá€­á€¯á€¡á€•á€ºá€•á€«á‹
        3.  **á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€›á€¾á€­á€€ á€á€”á€ºá€á€¶á€•á€«:** á€¡á€€á€šá€ºá á€•á€±á€¸á€‘á€¬á€¸á€á€±á€¬ Context á€á€Šá€º á€™á€±á€¸á€á€½á€”á€ºá€¸á€€á€­á€¯á€–á€¼á€±á€†á€­á€¯á€›á€”á€º á€œá€¯á€¶á€œá€±á€¬á€€á€ºá€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€•á€«á€á€„á€ºá€•á€«á€€ **"á€•á€±á€¸á€‘á€¬á€¸á€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸á€¡á€› á€¤á€™á€±á€¸á€á€½á€”á€ºá€¸á€€á€­á€¯ á€–á€¼á€±á€†á€­á€¯á€”á€­á€¯á€„á€ºá€á€¼á€„á€ºá€¸á€™á€›á€¾á€­á€•á€«"** á€Ÿá€¯ á€á€­á€€á€»á€…á€½á€¬á€–á€¼á€±á€†á€­á€¯á€•á€«á‹ á€™á€™á€¾á€”á€ºá€€á€”á€ºá€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€™á€¾á€”á€ºá€¸á€†á **á€œá€¯á€¶á€¸á€á€™á€–á€¼á€±á€†á€­á€¯á€•á€«á€”á€¾á€„á€·á€º**á‹
        4.  **á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€–á€¼á€±á€†á€­á€¯á€á€¼á€„á€ºá€¸:** á€™á€±á€¸á€á€½á€”á€ºá€¸á€€ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¯á€¶á€¸á€á€•á€ºá€›á€”á€º á€œá€­á€¯á€¡á€•á€ºá€•á€«á€€ (á€¥á€•á€™á€¬ - á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€‰á€ºá€á€­á€¯á€„á€ºá€¸á€á€¼á€„á€ºá€¸)áŠ Context á€‘á€²á€™á€¾ á€á€€á€ºá€†á€­á€¯á€„á€ºá€›á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯á€á€¬ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€•á€¼á€®á€¸ á€¡á€–á€¼á€±á€á€…á€ºá€á€¯á€¡á€–á€¼á€…á€º á€•á€¼á€”á€ºá€œá€Šá€ºá€–á€½á€²á€·á€…á€Šá€ºá€¸á€•á€«á‹ Context á€á€½á€„á€ºá€™á€•á€«á€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€¡á€á€…á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€™á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€•á€«á€”á€¾á€„á€·á€ºá‹

        =========================
        **á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€°á á€™á€±á€¸á€á€½á€”á€ºá€¸:** "{user_query}"
        =========================

        **á€›á€¾á€¬á€–á€½á€±á€á€½á€±á€·á€›á€¾á€­á€‘á€¬á€¸á€á€±á€¬ Report á€™á€»á€¬á€¸:**
        {context_for_prompt}
        =========================

        **á€á€„á€ºá á€¡á€–á€¼á€±:**
    """    
    
    # Call Gemini to get the best response
    client = genai.Client(api_key=GOOGLE_API_KEY)
    response = client.models.generate_content(
        model='gemini-2.0-flash-001', contents=f'{prompt}',
    )

    return response.text.strip()

def run_rag_query(user_query: str) -> str:
   
    similar_results = find_most_similar_result(user_query)
    
    if not similar_results:
        return "á€™á€±á€¸á€á€½á€”á€ºá€¸á€”á€¾á€„á€·á€º á€†á€€á€ºá€…á€•á€ºá€á€±á€¬ Report á€™á€»á€¬á€¸á€€á€­á€¯ á€›á€¾á€¬á€™á€á€½á€±á€·á€•á€«á‹"

    final_choice = find_best_response_with_gemini(user_query, similar_results)
    

    print("\n==============================================")
    print(f"ğŸ‘¤  User Query: \"{user_query}\"")
    print(f"ğŸ¤–  Cortex's Final Recommendation: {final_choice}")
    print("==============================================")

    return final_choice.strip()

